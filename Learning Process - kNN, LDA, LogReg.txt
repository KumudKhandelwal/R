
# The learning process

So, we saw that it is important to evaluate (in the right way) the predictors we learn from the data. We are going to illustrate in practice the different concepts.

```{r}
x = runif(100,0,10)
eps = rnorm(100,0,1.5)
y = 0.5 * x - 1.2 * x^2 + 0.1 * x^3 + eps
plot(x,y,type='p',pch=19)
```


Let us first try to fit a model and compute the (learning) error (in a naive way):

```{r}
# Learning step
f = lm(y ~ x)
f
plot(x,y,type='p')
abline(f)

# Predict for some data (here the learning data again)
yhat = predict(f,newdata = as.data.frame(x))
plot(y,yhat,type='p')
```

```{r}
# Compute the learning error
ErrL = sum((y - yhat)^2) / length(yhat)
ErrL
```

> Rmk: even though this error is too optimistic, because computed on the same data that were used for learning the model, this error is not perfect because the model here is very simple.


Let us try now a very complex model to see if we can overfit the data:

```{r}
# Learning step
X = data.frame(x,x^2,x^3,x^4,x^5,x^6)
f6 = lm(y ~ .,data = X)

# Predict for some data (here the learning data again)
yhat = predict(f6,newdata = X)
ErrL = sum((y - yhat)^2) / length(yhat)
ErrL
```


Let us now use the minimal setup to estimate the error of f:

```{r}
# Split between train and validation
train = sample(1:100,75)

# Learning step
f2 = lm(y ~ x,subset = train)
plot(x,y,type='p')
abline(f)
abline(f2,col='red')

# Predict for some data (here the learning data again)
xval = data.frame(x = x[-train])
yhat = predict(f2,newdata = xval)
# yhatL = predict(f2,newdata = data.frame(x = x[train]))
# ErrL = sum((y[train] - yhatL)^2) / length(yhat)
# ErrL
ErrV = sum((y[-train] - yhat)^2) / length(yhat)
ErrV
```


Let us do the same for the very complex model (f6):

```{r}
X = data.frame(x,x^2,x^3,x^4,x^5,x^6)

# Split between train and validation
train = sample(1:100,75)

# Learning step
f6 = lm(y ~ .,data = X,subset = train)

# Predict for some data (here the learning data again)
Xval = X[-train,]
yhat = predict(f6,newdata = Xval)
ErrV = sum((y[-train] - yhat)^2) / length(yhat)
ErrV
```


## The overfitting effect and the selection of the model with the right complexity

So, we are going to explore models ranging from very simple to very complex ones:

```{r}
x = runif(1000,0,10)
eps = rnorm(1000,0,1.5)
y = 0.5 * x - 1.2 * x^2 + 0.1 * x^3 + eps

ErrL = ErrV = rep(NA,20)
X = data.frame(x)
for (c in 2:20){
  X = data.frame(X,x^c) # we add a new variable at each iteration, x^c

  # Estimation and evaluation on the whole data set (bad way!)
  f = lm(y ~ .,data = X)
  yhat = predict(f,newdata = X)
  ErrL[c] = sum((y - yhat)^2) / length(yhat)
  
  # Estimation and evaluation on train and validation (good way!)
  train = sample(1:1000,75)
  f2 = lm(y ~ .,data = X,subset = train)
  Xval = X[-train,]
  yhat = predict(f2,newdata = Xval)
  ErrV[c] = sum((y[-train] - yhat)^2) / length(yhat)
}

plot(ErrL,type='b')
lines(ErrV,type='b',col='red')
```


```{r}
which.min(ErrL)
which.min(ErrV)
```

Finally, let us again improve the selection of the right model by using cross-validation.

Here we are first using CV for one specific model:

```{r}
X = data.frame(x,x^2,x^3)

ErrV = c()
# Leave-one-out CV 
for (i in 1:nrow(X)){
  # Split between train and validation
  train = seq(1,nrow(X))[-i]

  # Learning step
  f3 = lm(y ~ .,data = X,subset = train)
  
  # Predict for some data (here the learning data again)
  Xval = X[i,]
  yhat = predict(f3,newdata = Xval)
  ErrV[i] = sum((y[i] - yhat)^2) / length(yhat)
}
boxplot(ErrV)
median(ErrV)
```

Now, we can reproduce the procedure for choosing the best model, using this time CV instead of a simple split:


```{r}
x = runif(1000,0,10)
eps = rnorm(1000,0,1.5)
y = 0.5 * x - 1.2 * x^2 + 0.1 * x^3 + eps

ErrL = ErrV = ErrCV = rep(NA,10)
X = data.frame(x)
ErrCV.all = matrix(NA,1000,10)
for (c in 2:10){
  X = data.frame(X,x^c) # we add a new variable at each iteration, x^c

  # Estimation and evaluation on the whole data set (bad way!)
  f = lm(y ~ .,data = X)
  yhat = predict(f,newdata = X)
  ErrL[c] = sum((y - yhat)^2) / length(yhat)
  
  # Estimation and evaluation on train and validation (good way!)
  train = sample(1:1000,75)
  f2 = lm(y ~ .,data = X,subset = train)
  Xval = X[-train,]
  yhat = predict(f2,newdata = Xval)
  ErrV[c] = sum((y[-train] - yhat)^2) / length(yhat)
  
  # Estimation of the error with LOO-CV
  for (i in 1:nrow(X)){
    # Split between train and validation
    train = seq(1,nrow(X))[-i]
  
    # Learning step
    f3 = lm(y ~ .,data = X,subset = train)
    
    # Predict for some data (here the learning data again)
    Xval = X[i,]
    yhat = predict(f3,newdata = Xval)
    ErrCV.all[i,c] = sum((y[i] - yhat)^2) / length(yhat)
  }
  ErrCV[c] = mean(ErrCV.all[,c])
}

plot(ErrL,type='b')
lines(ErrV,type='b',col='red')
lines(ErrCV,type='b',col='green')

boxplot(ErrCV.all)
```

```{r}
which.min(ErrL)
which.min(ErrV)
which.min(ErrCV)
```


> Exercise: write a code that allows to select the right model and for the selected model learn the final predictor.

```{r}
x = runif(1000,0,10)
eps = rnorm(1000,0,1.5)
y = 0.5 * x - 1.2 * x^2 + 0.1 * x^3 + eps

ErrCV = rep(NA,10)
X = data.frame(x)
ErrCV.all = matrix(NA,1000,10)
for (c in 2:10){
  X = data.frame(X,x^c) # we add a new variable at each it, x^c
  
  # Estimation of the error with LOO-CV
  for (i in 1:nrow(X)){
    # Split between train and validation
    train = seq(1,nrow(X))[-i]
  
    # Learning step
    f3 = lm(y ~ .,data = X,subset = train)

# Predict for some data (here the learning data again)
    Xval = X[i,]
    yhat = predict(f3,newdata = Xval)
    ErrCV.all[i,c] = sum((y[i] - yhat)^2) / length(yhat)
  }
  ErrCV[c] = mean(ErrCV.all[,c])
}

ind = which.min(ErrCV)
cat('The selected model is of complexity',ind,'\n')

# Learn the final model
X = X[,1:ind]
f = lm(y ~ .,data = X)
```


# Classification with kNN, logistic regression and LDA

So, let us consider some data:

```{r message=FALSE}
#install.packages('MBCbook')
library(MBCbook)
data(wine27)
```


## KNN

Let us use kNN to try classify those data:

```{r}
N = nrow(wine27)
X = wine27[,1:27]
Y = wine27$Type
train = sample(1:N,150)

# Use knn to classify
library(class)
out = knn(X[train,],X[-train,],Y[train],k=3)
out
```

We can now compute the validation error:

```{r}
sum(out != Y[-train]) / length(out)
```

Let us try now the LOO-CV to get a better estimate of the error of kNN on these data:

```{r}
ErrCV = rep(NA,N)
# Estimation of the error with LOO-CV
for (i in 1:nrow(X)){
  # Split between train and validation
  train = seq(1,nrow(X))[-i]
  
  # Learning step
  out = knn(X[train,],X[-train,],Y[train],k=3)
  
  # compute the error
  ErrCV[i] = sum(out != Y[-train]) / length(out)
}
# here the boxplot is not interesting because the result set is either 0 or 1
# boxplot(ErrCV)
mean(ErrCV)
```
In classification, the error range is in between 0 and 1. So, we want to keep it as low as possible.
We can observe that the 3-NN produces an average classification error around 20%, which is not very satisfying. (In the future, we can wrongly classify 20% of the wines as per this outcome) A way to improve it is to test other values of k and to use CV for pick the most appropriate k for those data :

```{r}
ErrCV = matrix(NA,N,25)

for (k in 1:25){
  for (i in 1:nrow(X)){
    # Split between train and validation
    train = seq(1,nrow(X))[-i]
    
    # Learning step
    out = knn(X[train,],X[-train,],Y[train],k=k)
    
    # cmopute the error
    ErrCV[i,k] = sum(out != Y[-train]) / length(out)
  }
}
boxplot(ErrCV)
# plot all the 25 values of ErrCV for respective k (k=1,2,...,25)
plot(colMeans(ErrCV),type='b')
# find which one is minimum of all the ErrCV
which.min(colMeans(ErrCV))
```
```{r}
# see all the values of ErrCV
colMeans(ErrCV)
```



It turns out that the best solution that we can have with KNN is with k=9. Though, the resukt is still not very much satisfying as the error is as low as around 0-2%.

##LDA

Let us now use LDA to classify the same data:

```{r}
N = nrow(wine27)
X = wine27[,1:27]
Y = wine27$Type
train = sample(1:N,150)

# LDA learning step
f = lda(X[train,],Y[train])

# LDA prediction step
yhat = predict(f,X[-train,])

# f$prior - pi values of classes
# f$means - mean values of classes
# f$scaling - SIGMA (common covariance matrix)

# yhat$class - class for each observation
# yhat$posterior - the probabilities of all the validation data set for each class
# yhat$x - the mean of each partition set after LDA 

# Validation error
sum(yhat$class != Y[-train]) / length(yhat$class)
```

> Note: the estimated classifier `f` contains the maximum likelihood estimates for model parameters (f$prior = pi, f$means = mu, f$scaling = Sigma).


Let us now use LOO-CV to get a good estimate of the actual classification error of LDA for this wine classification problem :


```{r}
ErrCV = rep(NA,N)
# Estimation of the error with LOO-CV
for (i in 1:nrow(X)){
  # Split between train and validation
  train = seq(1,nrow(X))[-i]
  
  # Learning and classification step
  f = lda(X[train,],Y[train])
  yhat = predict(f,X[-train,])$class
  
  # cmopute the error
  ErrCV[i] = sum(yhat != Y[-train]) / length(yhat)
}

mean(ErrCV)
```

So, for this wine classification problem, LDA clearly outperforms kNN. This decision is based on the respective evaluations of the classification error with LOO-CV for LDA (0.01 ) and kNN (0.18 with k=9).


## Logistic regression

The logistic regression is available in R thanks to the `glm` function. Let's recall that this method is limited to binary classification. Let's therefore consider a different classification problem for now: the detection of counterfeit bank notes.

```{r}
data("banknote")
X = banknote[,-1]
Y = banknote$Status
N = nrow(X)
# Split into train / validation
train = sample(1:N,150)

# Learn the logistic regression model
f = glm(Status ~ .,data = banknote, subset = train, family = 'binomial')
# f = glm(Y ~ .,data = X, subset = train, family = 'binomial')

# Classify the validation data
out = predict(f,newdata = X[-train,])
yhat = as.numeric(out > 0) + 1

# Compute the classification error
sum(yhat != as.numeric(Y[-train])) / sum(yhat)
# sum(yhat != as.numeric(Y[-train])) / length(yhat)
```

So, finally, we can now compare on this problem kNN, LDA and logistic regression, thanks to LOO-CV. 
If we would like to do in the best way, we first have to select the most appropriate k for KNN on this problem :

```{r}
N = nrow(X)
ErrCV = matrix(NA,N,25)
for (k in 1:25){
  for (i in 1:nrow(X)){
    # Split between train and validation
    train = seq(1,nrow(X))[-i]
    
    # Learning step
    out = knn(X[train,],X[-train,],Y[train],k=k)
    
    # compute the error
    ErrCV[i,k] = sum(out != Y[-train]) / length(out)
  }
}

plot(colMeans(ErrCV),type='b')
which.min(colMeans(ErrCV))
```

And now, we can compare LDA, LReg and KNN (k=1)

```{r}
ErrCV.kNN = rep(NA,N)
ErrCV.LDA = rep(NA,N)
ErrCV.LReg = rep(NA,N)
# Estimation of the error with LOO-CV
for (i in 1:nrow(X)){
  # Split between train and validation
  train = seq(1,nrow(X))[-i]
  
  # LDA
  f = lda(X[train,],Y[train])
  yhat = predict(f,X[-train,])$class
  ErrCV.LDA[i] = sum(yhat != Y[-train]) / length(yhat)
  
  # Logistic regression
  f = glm(Status ~ .,data = banknote, subset = train,family = 'binomial')
  out = predict(f,newdata = X[-train,])
  yhat = as.numeric(out > 0) + 1 
  ErrCV.LReg[i] = sum(yhat != as.numeric(Y[-train])) / sum(yhat)
  
  # KNN (k = 1)
  out = knn(X[train,],X[-train,],Y[train],k=1)
  ErrCV.kNN[i] = sum(out != Y[-train]) / length(out)
}
Err = cbind(ErrCV.kNN,ErrCV.LDA,ErrCV.LReg)
# boxplot(Err, names=c('kNN', 'LDA', 'LReg'))
colMeans(Err)
apply(Err,2,sd)
```

Based on those results, we can recommend to put in production either 1-NN or LDA to classify the banknotes, and we expect a future classification error of 0.5% (sd 0.07).

## Application to the Swiss data

```{r}
data(swiss)
swiss

X = swiss[,-5]
Y = swiss$Catholic >= 50
```

> Exercise: compare KNN, LDA and LogReg for classifying those data, thanks to LOO-CV.


```{r message=FALSE, warning=FALSE}
data(swiss)
swiss
# Compute if the city has a majority of catholic people or not
X = swiss[,-5]
Y = as.numeric(swiss$Catholic >= 50)
S = X; S$Y = Y

# Choice of k for kNN
N = nrow(X)
ErrCV = matrix(NA,N,25)
for (k in 1:25){
  for (i in 1:nrow(X)){
    # Split between train and validation
    train = seq(1,nrow(X))[-i]
    
    # Learning step
    out = knn(X[train,],X[-train,],Y[train],k=k)
    
    # compute the error
    ErrCV[i,k] = sum(out != Y[-train]) / length(out)
  }
}

plot(colMeans(ErrCV),type='b')
kstar = which.min(colMeans(ErrCV))

# Comparison of the three methods
ErrCV.kNN = rep(NA,N)
ErrCV.LDA = rep(NA,N)
ErrCV.LReg = rep(NA,N)
# Estimation of the error with LOO-CV
for (i in 1:nrow(X)){
  # Split between train and validation
  train = seq(1,nrow(X))[-i]
  
  # LDA
  f = lda(X[train,],Y[train])
  yhat = predict(f,X[-train,])$class
  ErrCV.LDA[i] = sum(yhat != Y[-train])
  
  # Logistic regression
  f = glm(Y ~ .,data = S, subset = train,family = 'binomial')
  out = predict(f,newdata = X[-train,])
  yhat = as.numeric(out > 0)
  # ErrCV.LReg[i] = sum(yhat != Y[-train]) / sum(yhat)
  ErrCV.LReg[i] = sum(yhat != Y[-train])
  # KNN
  out = knn(X[train,],X[-train,],Y[train],k=kstar)
  ErrCV.kNN[i] = sum(out != Y[-train]) / length(out)
}
Err = cbind(ErrCV.kNN,ErrCV.LDA,ErrCV.LReg)
colMeans(Err)
apply(Err,2,sd)
```